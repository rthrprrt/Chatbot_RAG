# Chatbot RAG Application

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

This repository contains a chatbot application that uses Retrieval-Augmented Generation (RAG) to answer questions based on a provided PDF document. It features a user interface built with Gradio and leverages the LangChain ecosystem for document processing, embedding, retrieval, and conversational AI.

## Features

*   **RAG Pipeline:** Answers questions using context retrieved from a specified PDF document.
*   **Conversational Memory:** Remembers previous interactions within a session.
*   **Gradio UI:** Provides an interactive web interface for chatting.
*   **Configurable:** Uses environment variables (`.env` file) for API keys, PDF paths, prompts, and model settings.
*   **Streaming Responses:** Displays responses chunk by chunk as they are generated by the underlying model (simulated in UI).
*   **Modular Design:** Separates UI (`app.py`) from core chatbot logic (`src/chatbot_logic.py`).

## How RAG Works Here

1.  **Load & Split:** The specified PDF document (`PDF_PATH` in `.env`) is loaded and split into smaller chunks.
2.  **Embed & Store:** Each chunk is converted into a numerical representation (embedding) using OpenAI's models and stored in a Chroma vector database (in-memory by default, or persisted if `VECTORSTORE_PATH` is set).
3.  **Retrieve:** When you ask a question, the chatbot first checks the chat history. It may reformulate your question to be standalone. Then, it converts the question into an embedding and searches the vector database for the most relevant document chunks.
4.  **Generate:** The original question, chat history, and the retrieved document chunks are passed to the OpenAI language model (`gpt-3.5-turbo-0125` by default). The model uses this information to generate an informed answer.

## Project Structure

Use code with caution.
Markdown
Chatbot_RAG/
├── src/
│ ├── init.py
│ ├── config.py # Handles loading configuration (.env)
│ └── chatbot_logic.py # Core RAG chain setup and logic
├── data/ # Directory for input documents (add to .gitignore if needed)
│ └── example.pdf # Example placeholder PDF
├── app.py # Main Gradio application
├── requirements.txt # Python dependencies
├── .env_example # Example environment variables file
├── .gitignore # Git ignore file
├── LICENSE # License file (MIT)
└── README.md # This documentation file

## Installation

### Prerequisites

*   Python 3.8+
*   pip
*   Git

### Steps

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/rthrprrt/Chatbot_RAG.git
    cd Chatbot_RAG
    ```

2.  **Create a Virtual Environment:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # Linux/macOS
    # .venv\Scripts\activate    # Windows
    ```

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure Environment Variables:**
    *   Copy the example file:
        ```bash
        cp .env_example .env
        ```
    *   **Edit the `.env` file:**
        *   Set your `OPENAI_API_KEY`.
        *   Update `PDF_PATH` to point to the PDF document you want the chatbot to use (e.g., `data/my_document.pdf`). You might need to create the `data` directory and place your PDF there.
        *   (Optional) Customize `QA_SYSTEM_PROMPT`, `OPENAI_MODEL_NAME`, chunk settings, or `VECTORSTORE_PATH`.

## Usage

1.  **Activate the Virtual Environment:**
    ```bash
    source .venv/bin/activate  # Or the equivalent for your OS
    ```

2.  **Run the Gradio Application:**
    ```bash
    python app.py
    ```

3.  Open your web browser and navigate to the local URL provided by Gradio (usually `http://127.0.0.1:7860`).

4.  Interact with the chatbot through the web interface.

## Dependencies

Key libraries used:

*   `gradio`: For building the web UI.
*   `langchain`, `langchain-core`, `langchain-openai`, `langchain-community`, `langchain-chroma`, `langchain-text-splitters`: Core components for the RAG pipeline, LLM interaction, document handling, and vector storage.
*   `pypdf`: For loading PDF documents.
*   `python-dotenv`: For managing environment variables.
*   `openai`: Underlying library for OpenAI API calls (via LangChain).
*   `chromadb`: Implicit dependency for `langchain-chroma`.

See `requirements.txt` for the full list and specific versions.

## Limitations

*   **In-Memory History:** The chat history is stored in memory and will be lost when the application restarts. For persistent history, the `_session_store` in `src/chatbot_logic.py` needs to be replaced with a database or other persistent storage mechanism.
*   **Session Management:** Uses simple UUIDs for session IDs per connection. Does not handle user accounts or long-term persistence across browser sessions robustly.
*   **Scalability:** The in-memory vector store and history limit scalability. For larger documents or more users, consider database-backed vector stores (e.g., PostgreSQL with pgvector, Weaviate) and history stores (e.g., Redis, SQL database).
*   **Error Handling:** Basic error handling is included, but could be made more comprehensive.

## Contributing

Contributions, issues, and feature requests are welcome!

1.  Fork the repository.
2.  Create your feature branch (`git checkout -b feature/AmazingFeature`).
3.  Commit your changes (`git commit -m 'Add some AmazingFeature'`).
4.  Push to the branch (`git push origin feature/AmazingFeature`).
5.  Open a Pull Request.

## License

Distributed under the MIT License. See `LICENSE` for more information.